# AnÃ¡lise de EmoÃ§Ãµes e Atividades em VÃ­deo

Este projeto realiza a detecÃ§Ã£o automÃ¡tica de **emoÃ§Ãµes faciais** e **atividades corporais** em vÃ­deos, utilizando bibliotecas como `DeepFace`, `MediaPipe`, `OpenCV` e `NumPy`. O resultado Ã© salvo como um novo vÃ­deo com sobreposiÃ§Ãµes visuais (emoÃ§Ãµes) e um **relatÃ³rio em JSON** com estatÃ­sticas detalhadas.

---

## ğŸ“Œ Funcionalidades

- ğŸ­ DetecÃ§Ã£o de emoÃ§Ãµes como *happy*, *sad*, *angry*, etc.
- ğŸ•º Reconhecimento de atividades: `moving`, `walking`, `hands_up`, `stopped`, `dancing`, etc.
- âš ï¸ DetecÃ§Ã£o de movimentos anÃ´malos (ex: gestos bruscos).
- ğŸ“Š GeraÃ§Ã£o de resumo em JSON com contagem por emoÃ§Ã£o e atividade.
- ğŸ¥ ExportaÃ§Ã£o de um novo vÃ­deo com rÃ³tulos visuais por face appada.

---

## ğŸ§° Requisitos

- Python â‰¥ 3.8  
- OpenCV (com suporte a vÃ­deo)  
- MediaPipe  
- DeepFace  
- tqdm  
- NumPy

---

## ğŸ› ï¸ InstalaÃ§Ã£o

1. Clone este repositÃ³rio:

```bash
git clone https://github.com/andersoncatao/fiap_tc_f4.git
cd fiap_tc_f4
```

2. Crie um ambiente virtual (opcional, mas recomendado):

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate      # Windows
```

3. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Como Usar

1. Coloque seu vÃ­deo na raiz do projeto ou indique o caminho.
2. Execute o script principal:

```bash
python app.py
```

Por padrÃ£o, ele analisa o vÃ­deo `input_video.mp4` e gera:

- `output_video.mp4` com caixas e rÃ³tulos de emoÃ§Ã£o
- `summary.json` com estatÃ­sticas como:

```json
{
  "video": "input_video.mp4",
  "total_frames": 3326,
  "frames_processed": 3326,
  "anomalies_apped": 12,
  "emotions": {
    "happy": 1003,
    "neutral": 530
  },
  "activities": {
    "walking": 1020,
    "hands_up": 320,
    "dancing": 50
  }
}
```

---

## âš™ï¸ ParÃ¢metros personalizÃ¡veis

VocÃª pode alterar no cÃ³digo:

- `appor_backend='retinaface'` â†’ para `'mediapipe'`, `'opencv'`, etc.
- `app_every_n=10` â†’ quantos quadros pular entre detecÃ§Ãµes

---

## ğŸ“‚ Estrutura do Projeto

```
.
â”œâ”€â”€ app.py                 # Script principal
â”œâ”€â”€ output_video.mp4       # (Gerado apÃ³s execuÃ§Ã£o)
â”œâ”€â”€ summary.json           # (Gerado apÃ³s execuÃ§Ã£o)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## â— ObservaÃ§Ãµes

- DeepFace pode baixar modelos na primeira execuÃ§Ã£o (pasta `.deepface`).
- `cv2.TrackerCSRT_create()` requer OpenCV com contribs â€” jÃ¡ incluÃ­do no `opencv-python` comum.

---

## ğŸ§  CrÃ©ditos

- [DeepFace](https://github.com/serengil/deepface) â€“ AnÃ¡lise facial
- [MediaPipe](https://google.github.io/mediapipe/) â€“ DetecÃ§Ã£o de pose
- [OpenCV](https://opencv.org/) â€“ Processamento de vÃ­deo

---

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.
